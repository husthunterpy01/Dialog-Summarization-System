{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6004344,"sourceType":"datasetVersion","datasetId":3438844},{"sourceId":10000750,"sourceType":"datasetVersion","datasetId":6155682}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\n\nsamsum_train_dataset = load_dataset(\"csv\", data_files={\"train\": \"/kaggle/input/samsum-dataset-text-summarization/samsum-test.csv\"})\nsamsum_test_dataset = load_dataset(\"csv\", data_files={\"test\": \"/kaggle/input/samsum-dataset-text-summarization/samsum-test.csv\"})\nsamsum_validate_dataset = load_dataset(\"csv\", data_files={\"validation\": \"/kaggle/input/samsum-dataset-text-summarization/samsum-validation.csv\"})\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T14:50:29.380761Z","iopub.execute_input":"2024-11-25T14:50:29.381153Z","iopub.status.idle":"2024-11-25T14:50:29.774615Z","shell.execute_reply.started":"2024-11-25T14:50:29.381123Z","shell.execute_reply":"2024-11-25T14:50:29.773965Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"samsum_train_dataset[\"train\"][1]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T14:50:35.374562Z","iopub.execute_input":"2024-11-25T14:50:35.375300Z","iopub.status.idle":"2024-11-25T14:50:35.382365Z","shell.execute_reply.started":"2024-11-25T14:50:35.375266Z","shell.execute_reply":"2024-11-25T14:50:35.381429Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"{'id': '13729565',\n 'dialogue': \"Eric: MACHINE!\\r\\nRob: That's so gr8!\\r\\nEric: I know! And shows how Americans see Russian ;)\\r\\nRob: And it's really funny!\\r\\nEric: I know! I especially like the train part!\\r\\nRob: Hahaha! No one talks to the machine like that!\\r\\nEric: Is this his only stand-up?\\r\\nRob: Idk. I'll check.\\r\\nEric: Sure.\\r\\nRob: Turns out no! There are some of his stand-ups on youtube.\\r\\nEric: Gr8! I'll watch them now!\\r\\nRob: Me too!\\r\\nEric: MACHINE!\\r\\nRob: MACHINE!\\r\\nEric: TTYL?\\r\\nRob: Sure :)\",\n 'summary': 'Eric and Rob are going to watch a stand-up on youtube.'}"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"print(samsum_test_dataset.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T14:50:39.099584Z","iopub.execute_input":"2024-11-25T14:50:39.099944Z","iopub.status.idle":"2024-11-25T14:50:39.105983Z","shell.execute_reply.started":"2024-11-25T14:50:39.099914Z","shell.execute_reply":"2024-11-25T14:50:39.105112Z"}},"outputs":[{"name":"stdout","text":"dict_keys(['test'])\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"from transformers import pipeline\n\ntext_summarizer = pipeline(\"summarization\", model=\"facebook/bart-base\", device=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T14:50:41.751964Z","iopub.execute_input":"2024-11-25T14:50:41.752327Z","iopub.status.idle":"2024-11-25T14:50:42.935352Z","shell.execute_reply.started":"2024-11-25T14:50:41.752288Z","shell.execute_reply":"2024-11-25T14:50:42.934401Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"text_summarizer(samsum_train_dataset[\"train\"][1][\"dialogue\"], max_length=20, min_length=10, do_sample= False )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T14:51:00.577808Z","iopub.execute_input":"2024-11-25T14:51:00.578653Z","iopub.status.idle":"2024-11-25T14:51:00.730230Z","shell.execute_reply.started":"2024-11-25T14:51:00.578618Z","shell.execute_reply":"2024-11-25T14:51:00.729364Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"[{'summary_text': \"Eric: MACHINE! Palestin Palestin PalestinescriptionRob: That's so gr8\"}]"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"# Fine tune the SamSUM model to improve the summarize performance\n# Add the BART tokenizer and model\nfrom transformers import BartForConditionalGeneration, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\nmodel = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\", dropout=0.3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T14:51:13.087119Z","iopub.execute_input":"2024-11-25T14:51:13.087475Z","iopub.status.idle":"2024-11-25T14:51:15.205069Z","shell.execute_reply.started":"2024-11-25T14:51:13.087444Z","shell.execute_reply":"2024-11-25T14:51:15.204131Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"def preprocessData(records, tokenizer, max_length_preprocess=128):\n    sources = records[\"dialogue\"]\n    targets = records[\"summary\"]\n\n    input_encoding = tokenizer(sources, max_length=max_length_preprocess, padding=\"max_length\", truncation=True)\n    with tokenizer.as_target_tokenizer():\n        output_encoding = tokenizer(targets, max_length=max_length_preprocess, padding=\"max_length\", truncation=True)\n\n    # Return as lists to ensure compatibility with DataLoader\n    return {\n        \"input_ids\": input_encoding[\"input_ids\"],\n        \"attention_mask\": input_encoding[\"attention_mask\"],\n        \"labels\": output_encoding[\"input_ids\"],\n    }\n\ntrain_dataset = samsum_train_dataset[\"train\"].map(lambda x: preprocessData(x, tokenizer), batched=True)\nvalidation_dataset = samsum_validate_dataset[\"validation\"].map(lambda x: preprocessData(x, tokenizer), batched=True)\ntest_dataset = samsum_test_dataset[\"test\"].map(lambda x: preprocessData(x, tokenizer), batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T14:51:16.329256Z","iopub.execute_input":"2024-11-25T14:51:16.330171Z","iopub.status.idle":"2024-11-25T14:51:16.441276Z","shell.execute_reply.started":"2024-11-25T14:51:16.330118Z","shell.execute_reply":"2024-11-25T14:51:16.440532Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"# Build the customized DataLoader class for fine-tunning\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset\n\nclass SamSUMDataset(Dataset):\n    def __init__(self, tokenizer, max_length=512):\n        self.dataset = samsum_train_dataset['train']\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        dialogue = self.dataset[idx]['dialogue']\n        summary = self.dataset[idx]['summary']\n        inputs = self.tokenizer(\n            dialogue,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        labels = self.tokenizer(\n            summary,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        return {\n            \"input_ids\": inputs['input_ids'].squeeze(0),\n            \"attention_mask\": inputs['attention_mask'].squeeze(0),\n            \"labels\": labels['input_ids'].squeeze(0)\n        }\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nvalidation_dataloader = DataLoader(validation_dataset, batch_size=8)\ntest_dataloader = DataLoader(test_dataset, batch_size=8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T14:51:31.418662Z","iopub.execute_input":"2024-11-25T14:51:31.419532Z","iopub.status.idle":"2024-11-25T14:51:31.427884Z","shell.execute_reply.started":"2024-11-25T14:51:31.419494Z","shell.execute_reply":"2024-11-25T14:51:31.427005Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T14:51:33.488793Z","iopub.execute_input":"2024-11-25T14:51:33.489140Z","iopub.status.idle":"2024-11-25T14:51:41.963350Z","shell.execute_reply.started":"2024-11-25T14:51:33.489110Z","shell.execute_reply":"2024-11-25T14:51:41.962494Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.3)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"from transformers import BartForConditionalGeneration, TrainingArguments, Trainer, EarlyStoppingCallback\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"steps\",  \n    save_strategy=\"steps\",        \n    learning_rate=5e-5,\n    weight_decay= 0.01,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=10,\n    save_total_limit=2,\n    load_best_model_at_end=True,  \n    metric_for_best_model=\"eval_loss\", \n    greater_is_better=False,  \n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=validation_dataset,\n    tokenizer=tokenizer,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n)\n\n# Train the model\ntrainer.train()\n\n# Save the fine-tuned model and tokenizer\nmodel.save_pretrained(\"./finetuned_bart_samsum\")\ntokenizer.save_pretrained(\"./finetuned_bart_samsum\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T15:31:26.461918Z","iopub.execute_input":"2024-11-25T15:31:26.462703Z","iopub.status.idle":"2024-11-25T15:35:11.708409Z","shell.execute_reply.started":"2024-11-25T15:31:26.462659Z","shell.execute_reply":"2024-11-25T15:35:11.707522Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1030' max='1030' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1030/1030 03:32, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.167000</td>\n      <td>0.604076</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.162100</td>\n      <td>0.602450</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"('./finetuned_bart_samsum/tokenizer_config.json',\n './finetuned_bart_samsum/special_tokens_map.json',\n './finetuned_bart_samsum/vocab.json',\n './finetuned_bart_samsum/merges.txt',\n './finetuned_bart_samsum/added_tokens.json',\n './finetuned_bart_samsum/tokenizer.json')"},"metadata":{}}],"execution_count":72},{"cell_type":"code","source":"# Evaluate the model\nresults = trainer.evaluate(eval_dataset=test_dataset)\nprint(results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T15:35:11.709446Z","iopub.execute_input":"2024-11-25T15:35:11.709723Z","iopub.status.idle":"2024-11-25T15:35:17.127023Z","shell.execute_reply.started":"2024-11-25T15:35:11.709696Z","shell.execute_reply":"2024-11-25T15:35:17.126182Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='103' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [103/103 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.10316388309001923, 'eval_runtime': 5.4063, 'eval_samples_per_second': 151.489, 'eval_steps_per_second': 19.052, 'epoch': 10.0}\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"!pip install rouge_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T15:38:27.430925Z","iopub.execute_input":"2024-11-25T15:38:27.431194Z","iopub.status.idle":"2024-11-25T15:38:35.827867Z","shell.execute_reply.started":"2024-11-25T15:38:27.431167Z","shell.execute_reply":"2024-11-25T15:38:35.827009Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: rouge_score in /opt/conda/lib/python3.10/site-packages (0.1.2)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"# Model evaluating using ROUGE\nfrom evaluate import load\nimport torch\n\n# Load ROUGE metric\nrouge = load(\"rouge\")\n\n# Function to generate predictions\ndef generate_predictions(model, tokenizer, dataset):\n    predictions = []\n    references = []\n\n    for example in dataset:\n        # Prepare the input dialogue\n        inputs = tokenizer(\n            example[\"dialogue\"], \n            return_tensors=\"pt\", \n            max_length=512, \n            truncation=True, \n            padding=\"max_length\"\n        )\n        \n        # Move inputs to GPU if available\n        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()} if torch.cuda.is_available() else inputs\n        \n        # Generate summary\n        with torch.no_grad():\n            outputs = model.generate(\n                input_ids=inputs[\"input_ids\"], \n                attention_mask=inputs[\"attention_mask\"], \n                max_length=128, \n                min_length=30, \n                do_sample=False\n            )\n        \n        # Decode the generated summary\n        generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        # Append generated summary and reference summary\n        predictions.append(generated_summary)\n        references.append(example[\"summary\"])\n    \n    return predictions, references\n\n# Generate predictions and references\ntest_predictions, test_references = generate_predictions(model, tokenizer, validation_dataloader)\n\n# Compute ROUGE scores\nrouge_results = rouge.compute(predictions=test_predictions, references=test_references)\n\n# Print ROUGE scores\nprint(\"ROUGE Scores:\")\nfor key, value in rouge_results.items():\n    print(f\"{key}: {value:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T15:36:43.574623Z","iopub.execute_input":"2024-11-25T15:36:43.575291Z","iopub.status.idle":"2024-11-25T15:38:27.429134Z","shell.execute_reply.started":"2024-11-25T15:36:43.575256Z","shell.execute_reply":"2024-11-25T15:38:27.428217Z"}},"outputs":[{"name":"stdout","text":"ROUGE Scores:\nrouge1: 0.4243\nrouge2: 0.1841\nrougeL: 0.3337\nrougeLsum: 0.3335\n","output_type":"stream"}],"execution_count":75},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}