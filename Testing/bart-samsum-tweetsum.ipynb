{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6004344,"sourceType":"datasetVersion","datasetId":3438844},{"sourceId":10000750,"sourceType":"datasetVersion","datasetId":6155682},{"sourceId":10021056,"sourceType":"datasetVersion","datasetId":6170631}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\n\nsamsum_train_dataset = load_dataset(\"csv\", data_files={\"train\": \"/kaggle/input/samsum-dataset-text-summarization/samsum-test.csv\"})\nsamsum_test_dataset = load_dataset(\"csv\", data_files={\"test\": \"/kaggle/input/samsum-dataset-text-summarization/samsum-test.csv\"})\nsamsum_validate_dataset = load_dataset(\"csv\", data_files={\"validation\": \"/kaggle/input/samsum-dataset-text-summarization/samsum-validation.csv\"})\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T11:09:35.605167Z","iopub.execute_input":"2024-11-27T11:09:35.605901Z","iopub.status.idle":"2024-11-27T11:09:38.367879Z","shell.execute_reply.started":"2024-11-27T11:09:35.605843Z","shell.execute_reply":"2024-11-27T11:09:38.367012Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d4e1ab46ebc48c4b6f43bffbb497d5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c0df6dc83954ad49e94ee99123cb77c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae88142382d04d29924b56413af5a342"}},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"samsum_train_dataset[\"train\"][1]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T11:09:43.652169Z","iopub.execute_input":"2024-11-27T11:09:43.652909Z","iopub.status.idle":"2024-11-27T11:09:43.662816Z","shell.execute_reply.started":"2024-11-27T11:09:43.652860Z","shell.execute_reply":"2024-11-27T11:09:43.661323Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"{'id': '13729565',\n 'dialogue': \"Eric: MACHINE!\\r\\nRob: That's so gr8!\\r\\nEric: I know! And shows how Americans see Russian ;)\\r\\nRob: And it's really funny!\\r\\nEric: I know! I especially like the train part!\\r\\nRob: Hahaha! No one talks to the machine like that!\\r\\nEric: Is this his only stand-up?\\r\\nRob: Idk. I'll check.\\r\\nEric: Sure.\\r\\nRob: Turns out no! There are some of his stand-ups on youtube.\\r\\nEric: Gr8! I'll watch them now!\\r\\nRob: Me too!\\r\\nEric: MACHINE!\\r\\nRob: MACHINE!\\r\\nEric: TTYL?\\r\\nRob: Sure :)\",\n 'summary': 'Eric and Rob are going to watch a stand-up on youtube.'}"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"print(samsum_test_dataset.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T11:09:44.149667Z","iopub.execute_input":"2024-11-27T11:09:44.150033Z","iopub.status.idle":"2024-11-27T11:09:44.155743Z","shell.execute_reply.started":"2024-11-27T11:09:44.150000Z","shell.execute_reply":"2024-11-27T11:09:44.154563Z"}},"outputs":[{"name":"stdout","text":"dict_keys(['test'])\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from transformers import pipeline\n\ntext_summarizer = pipeline(\"summarization\", model=\"facebook/bart-base\", device=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T11:09:44.185102Z","iopub.execute_input":"2024-11-27T11:09:44.185390Z","iopub.status.idle":"2024-11-27T11:10:06.808751Z","shell.execute_reply.started":"2024-11-27T11:09:44.185364Z","shell.execute_reply":"2024-11-27T11:10:06.807776Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7317880298aa4935866ce3ebe01511c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b99daf48dcf14c1babbb303703a277cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d659a6cd7b5945e5912f6624e0e9570d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"678aed91c6064e1fbc83e40362d9a54c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcef5dafcc264606831906bcf2709eb0"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"text_summarizer(samsum_train_dataset[\"train\"][128][\"dialogue\"], max_length=20, min_length=10, do_sample= False )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T11:10:06.810570Z","iopub.execute_input":"2024-11-27T11:10:06.811776Z","iopub.status.idle":"2024-11-27T11:10:07.686345Z","shell.execute_reply.started":"2024-11-27T11:10:06.811733Z","shell.execute_reply":"2024-11-27T11:10:07.685515Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"[{'summary_text': 'Paul: hey Matthew did you find anyone to couch the game Saturday?Matthew: hey'}]"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Fine tune the SamSUM model to improve the summarize performance\n# Add the BART tokenizer and model\nfrom transformers import BartForConditionalGeneration, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\nmodel = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\", dropout=0.3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T11:10:07.687473Z","iopub.execute_input":"2024-11-27T11:10:07.687797Z","iopub.status.idle":"2024-11-27T11:10:09.229351Z","shell.execute_reply.started":"2024-11-27T11:10:07.687769Z","shell.execute_reply":"2024-11-27T11:10:09.228668Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Remove the icon tag like =), :v\n# Guess the word meaning for the missing character of a word\nimport re\n\ndef preprocess_missingchar_and_icon(sample):\n    def clean_text(text):\n        # Remove icon tags, including the characters inside angled brackets (e.g., <photo>, <emoji>)\n        text = re.sub(r'<.*?>', '', text)\n        \n        # Remove common emoticons or icons like :v, :-), :)\n        text = re.sub(r'(:\\)|:-\\)|:v|:D|<3)', '', text)\n             \n        # Remove extra whitespace caused by the removal of icons\n        text = re.sub(r'\\s+', ' ', text).strip()\n        \n        return text\n    \n    # Apply cleaning to the text and summary fields\n    sample[\"dialogue\"] = clean_text(sample[\"dialogue\"])\n    sample[\"summary\"] = clean_text(sample[\"summary\"])\n    return sample\n\nsamsum_train_dataset_clean = samsum_train_dataset.map(preprocess_missingchar_and_icon)\nsamsum_test_dataset_clean = samsum_test_dataset.map(preprocess_missingchar_and_icon)\nsamsum_validate_dataset_clean = samsum_validate_dataset.map(preprocess_missingchar_and_icon)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T11:10:09.231492Z","iopub.execute_input":"2024-11-27T11:10:09.231780Z","iopub.status.idle":"2024-11-27T11:10:09.548691Z","shell.execute_reply.started":"2024-11-27T11:10:09.231753Z","shell.execute_reply":"2024-11-27T11:10:09.547862Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/819 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a93d219ae9b64528a83e3aa8542ddcc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/819 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45950eb8adf94318a9c718cb2054ec79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/818 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a887e90cd004b5eba8d05b9314d51cb"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"def preprocessData(records, tokenizer, max_length_preprocess=128):\n    sources = records[\"dialogue\"]\n    targets = records[\"summary\"]\n\n    input_encoding = tokenizer(sources, max_length=max_length_preprocess*8, padding=\"max_length\", truncation=True)\n    with tokenizer.as_target_tokenizer():\n        output_encoding = tokenizer(targets, max_length=max_length_preprocess, padding=\"max_length\", truncation=True)\n\n    # Return as lists to ensure compatibility with DataLoader\n    return {\n        \"input_ids\": input_encoding[\"input_ids\"],\n        \"attention_mask\": input_encoding[\"attention_mask\"],\n        \"labels\": output_encoding[\"input_ids\"],\n    }\n\ntrain_dataset = samsum_train_dataset_clean[\"train\"].map(lambda x: preprocessData(x, tokenizer), batched=True)\nvalidation_dataset = samsum_validate_dataset_clean[\"validation\"].map(lambda x: preprocessData(x, tokenizer), batched=True)\ntest_dataset = samsum_test_dataset_clean[\"test\"].map(lambda x: preprocessData(x, tokenizer), batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T11:10:09.549707Z","iopub.execute_input":"2024-11-27T11:10:09.549968Z","iopub.status.idle":"2024-11-27T11:10:11.038766Z","shell.execute_reply.started":"2024-11-27T11:10:09.549943Z","shell.execute_reply":"2024-11-27T11:10:11.038046Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/819 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd7cbeb1bf3f4cd7b1871296e7dd5454"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4117: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/818 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e497b64f976147d3a145da0f432565d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/819 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1eacde9e8da0491d9dbaf6730b94b934"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Build the customized DataLoader class for fine-tunning\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nvalidation_dataloader = DataLoader(validation_dataset, batch_size=8)\ntest_dataloader = DataLoader(test_dataset, batch_size=8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T11:10:11.039690Z","iopub.execute_input":"2024-11-27T11:10:11.039920Z","iopub.status.idle":"2024-11-27T11:10:11.044758Z","shell.execute_reply.started":"2024-11-27T11:10:11.039896Z","shell.execute_reply":"2024-11-27T11:10:11.043967Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T11:10:11.045844Z","iopub.execute_input":"2024-11-27T11:10:11.046179Z","iopub.status.idle":"2024-11-27T11:10:20.616639Z","shell.execute_reply.started":"2024-11-27T11:10:11.046144Z","shell.execute_reply":"2024-11-27T11:10:20.615794Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from transformers import BartForConditionalGeneration, TrainingArguments, Trainer, EarlyStoppingCallback\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results/pre-trained-model\",\n    evaluation_strategy=\"steps\",  \n    save_strategy=\"steps\",        \n    learning_rate=5e-5,\n    weight_decay= 0.01,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=10,\n    save_total_limit=2,\n    load_best_model_at_end=True,  \n    metric_for_best_model=\"eval_loss\", \n    greater_is_better=False,  \n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=validation_dataset,\n    tokenizer=tokenizer,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n)\n\n# Train the model\ntrainer.train()\n\n# Save the fine-tuned model and tokenizer\nmodel.save_pretrained(\"./finetuned_bart_samsum\")\ntokenizer.save_pretrained(\"./finetuned_bart_samsum\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T11:10:20.618093Z","iopub.execute_input":"2024-11-27T11:10:20.618400Z","iopub.status.idle":"2024-11-27T11:37:46.326724Z","shell.execute_reply.started":"2024-11-27T11:10:20.618368Z","shell.execute_reply":"2024-11-27T11:37:46.325922Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113936288888402, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b4f6c6ce89f4b68a4635e02f09fab03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241127_112405-iqvn3p6f</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/zhukov01/huggingface/runs/iqvn3p6f' target=\"_blank\">./results/pre-trained-model</a></strong> to <a href='https://wandb.ai/zhukov01/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/zhukov01/huggingface' target=\"_blank\">https://wandb.ai/zhukov01/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/zhukov01/huggingface/runs/iqvn3p6f' target=\"_blank\">https://wandb.ai/zhukov01/huggingface/runs/iqvn3p6f</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1030' max='1030' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1030/1030 13:35, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.093400</td>\n      <td>0.416001</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.302100</td>\n      <td>0.424176</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"('./finetuned_bart_samsum/tokenizer_config.json',\n './finetuned_bart_samsum/special_tokens_map.json',\n './finetuned_bart_samsum/vocab.json',\n './finetuned_bart_samsum/merges.txt',\n './finetuned_bart_samsum/added_tokens.json',\n './finetuned_bart_samsum/tokenizer.json')"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Evaluate the model\nresults = trainer.evaluate(eval_dataset=test_dataset)\nprint(results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T11:37:46.327723Z","iopub.execute_input":"2024-11-27T11:37:46.328006Z","iopub.status.idle":"2024-11-27T11:38:07.270416Z","shell.execute_reply.started":"2024-11-27T11:37:46.327975Z","shell.execute_reply":"2024-11-27T11:38:07.269629Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='103' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [103/103 00:20]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.2509564757347107, 'eval_runtime': 20.9316, 'eval_samples_per_second': 39.128, 'eval_steps_per_second': 4.921, 'epoch': 10.0}\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!pip install rouge_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T11:38:07.272980Z","iopub.execute_input":"2024-11-27T11:38:07.273235Z","iopub.status.idle":"2024-11-27T11:38:17.260690Z","shell.execute_reply.started":"2024-11-27T11:38:07.273210Z","shell.execute_reply":"2024-11-27T11:38:17.259855Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=5ffa9fa381ef34c280b8e656869a43c55408f2d936a95f683ff235f4f3290dcf\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Model evaluating using ROUGE\nfrom evaluate import load\nimport torch\n\n# Load ROUGE metric\nrouge = load(\"rouge\")\n\n# Function to generate predictions\ndef generate_predictions(model, tokenizer, dataset):\n    predictions = []\n    references = []\n\n    for example in dataset:\n        # Prepare the input dialogue\n        inputs = tokenizer(\n            example[\"dialogue\"], \n            return_tensors=\"pt\", \n            max_length=512, \n            truncation=True, \n            padding=\"max_length\"\n        )\n        \n        # Move inputs to GPU if available\n        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()} if torch.cuda.is_available() else inputs\n        \n        # Generate summary\n        with torch.no_grad():\n            outputs = model.generate(\n                input_ids=inputs[\"input_ids\"], \n                attention_mask=inputs[\"attention_mask\"], \n                max_length=128, \n                min_length=30, \n                do_sample=False\n            )\n        \n        # Decode the generated summary\n        generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        # Append generated summary and reference summary\n        predictions.append(generated_summary)\n        references.append(example[\"summary\"])\n    \n    return predictions, references\n\n# Generate predictions and references\ntest_predictions, test_references = generate_predictions(model, tokenizer, validation_dataloader)\n\n# Compute ROUGE scores\nrouge_results = rouge.compute(predictions=test_predictions, references=test_references)\n\n# Print ROUGE scores\nprint(\"ROUGE Scores:\")\nfor key, value in rouge_results.items():\n    print(f\"{key}: {value:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T11:38:17.262075Z","iopub.execute_input":"2024-11-27T11:38:17.262383Z","iopub.status.idle":"2024-11-27T11:39:53.837047Z","shell.execute_reply.started":"2024-11-27T11:38:17.262353Z","shell.execute_reply":"2024-11-27T11:39:53.836160Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c98e72b34ed74a1783d6ca8bb3fccc3d"}},"metadata":{}},{"name":"stdout","text":"ROUGE Scores:\nrouge1: 0.4234\nrouge2: 0.1938\nrougeL: 0.3272\nrougeLsum: 0.3273\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"#Load the TweetSum dataset\nfrom datasets import load_dataset\n\ntweetsum_train = load_dataset(\"csv\", data_files={\"train\": \"/kaggle/input/tweetsum/tweetsum_train.csv\"})\ntweetsum_test = load_dataset(\"csv\", data_files={\"test\": \"/kaggle/input/tweetsum/tweetsum_test.csv\"})\ntweetsum_validate = load_dataset(\"csv\", data_files={\"validation\": \"/kaggle/input/tweetsum/tweetsum_valid.csv\"})\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T11:39:53.838139Z","iopub.execute_input":"2024-11-27T11:39:53.838416Z","iopub.status.idle":"2024-11-27T11:39:54.424988Z","shell.execute_reply.started":"2024-11-27T11:39:53.838390Z","shell.execute_reply":"2024-11-27T11:39:54.424148Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbab95ab98e8466b908e17a60eddd138"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8cb31076a6c4f8bbc1a5541baef5663"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f1a652a9ca44399a2f89f5e8c4a5e0c"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"tweetsum_train[\"train\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T11:39:54.426009Z","iopub.execute_input":"2024-11-27T11:39:54.426303Z","iopub.status.idle":"2024-11-27T11:39:54.432975Z","shell.execute_reply.started":"2024-11-27T11:39:54.426277Z","shell.execute_reply":"2024-11-27T11:39:54.432005Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"{'id': 1,\n 'dialogue': ' customer: neither my iPhone nor my Apple Watch are recording my steps/activity, and Health doesn’t recognise either source anymore for some reason. Any ideas?  customer: please read the above. support: Let’s investigate this together. To start, can you tell us the software versions your iPhone and Apple Watch are running currently? customer: My iPhone is on 11.1.2, and my watch is on 4.1. support: Thank you. Have you tried restarting both devices since this started happening? customer: I’ve restarted both, also un-paired then re-paired the watch. support: Got it. When did you first notice that the two devices were not talking to each other. Do the two devices communicate through other apps such as Messages? customer: Yes, everything seems fine, it’s just Health and activity. support: Let’s move to DM and look into this a bit more. When reaching out in DM, let us know when this first started happening please. For example, did it start after an update or after installing a certain app? ',\n 'summary': 'Customer enquired about his Iphone and Apple watch which is not showing his any steps/activity and health activities. Agent is asking to move to DM and look into it.'}"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"tweetsum_test[\"test\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T11:39:54.433950Z","iopub.execute_input":"2024-11-27T11:39:54.434284Z","iopub.status.idle":"2024-11-27T11:39:54.444622Z","shell.execute_reply.started":"2024-11-27T11:39:54.434243Z","shell.execute_reply":"2024-11-27T11:39:54.443834Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"{'id': 1,\n 'dialogue': \" customer: My watchlist is not updating with new episodes (past couple days).  Any idea why? support: Apologies for the trouble, Norlene! We're looking into this. In the meantime, try navigating to the season / episode manually. customer: Tried logging out/back in, that didn’t help support: Sorry! 😔 We assure you that our team is working hard to investigate, and we hope to have a fix ready soon! customer: Thank you! Some shows updated overnight, but others did not... support: We definitely understand, Norlene. For now, we recommend checking the show page for these shows as the new eps will be there customer: As of this morning, the problem seems to be resolved. Watchlist updated overnight with all new episodes. Thank you for your attention to this matter! I love Hulu 💚 support: Awesome! That's what we love to hear. If you happen to need anything else, we'll be here to support! 💚\",\n 'summary': 'Customer is complaining that the watchlist is not updated with new episodes from past two days. Agent informed that the team is working hard to investigate to show new episodes on page.'}"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"tweetsum_validate[\"validation\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T11:39:54.445730Z","iopub.execute_input":"2024-11-27T11:39:54.446059Z","iopub.status.idle":"2024-11-27T11:39:54.454134Z","shell.execute_reply.started":"2024-11-27T11:39:54.446022Z","shell.execute_reply":"2024-11-27T11:39:54.453377Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"{'id': 1,\n 'dialogue': ' customer: hey, any explanation why the \"Create similar playlist\" function doesn\\'t work anymore for me? MacBook, v1.0.64.399.g4637b02a. support: Hi there, the cavalry\\'s here! Does logging out, restarting your device, and logging back into Spotify help? Keep us in the loop /JI customer: no, it didn\\'t :( tried everything but I still can\\'t create the playlist. it\\'s not even greyed out but nothing happens after clicking on it. support: Okay. Can we have you try reinstalling the app? To do so, just follow the steps at  Let us know how it goes /JI customer: i tried and it\\'s still the same... moreover, my song history is always empty, so I can\\'t find songs from previous Discover playlists :( support: Does restarting your computer help at all? Also, is the song history you\\'re referring to the History tab on your Play Queue? /MT customer: no, I tried that as well and just reinstalled again - didn\\'t help. yes, that\\'s what I mean. support: Could you DM us your account\\'s email address or username? We\\'ll take a look backstage /MT ',\n 'summary': \"Customer is complaining about unable to create similar playlist so that  function does not  work anymore. Agent says could DM the account's email address or username so that they look backstage.\"}"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# Preprocessing with the TweetSUM dataset\ntweetsum_train_clean = tweetsum_train.map(preprocess_missingchar_and_icon)\ntweetsum_test_clean = tweetsum_test.map(preprocess_missingchar_and_icon)\ntweetsum_validate_clean = tweetsum_validate.map(preprocess_missingchar_and_icon)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T11:39:54.455095Z","iopub.execute_input":"2024-11-27T11:39:54.455551Z","iopub.status.idle":"2024-11-27T11:39:54.702860Z","shell.execute_reply.started":"2024-11-27T11:39:54.455525Z","shell.execute_reply":"2024-11-27T11:39:54.701757Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/879 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcbef26084344864bf23cdd0eab6c98b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/110 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"516d74171f8d4d15b4b151459f565d3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/110 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7c3cc2104ad4a7f8aee7c1279a4df5a"}},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# Load the BART_SamSUM model\n# Load the fine-tuned SAMSum model and tokenizer\nfrom transformers import BartForConditionalGeneration, BartTokenizer\n\nmodel_pretrained = BartForConditionalGeneration.from_pretrained(\"./finetuned_bart_samsum\")\ntokenizer_pretrained = BartTokenizer.from_pretrained(\"./finetuned_bart_samsum\")\nmodel_pretrained.resize_token_embeddings(len(tokenizer_pretrained))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T11:41:17.241501Z","iopub.execute_input":"2024-11-27T11:41:17.241886Z","iopub.status.idle":"2024-11-27T11:41:18.007052Z","shell.execute_reply.started":"2024-11-27T11:41:17.241853Z","shell.execute_reply":"2024-11-27T11:41:18.006236Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"BartScaledWordEmbedding(50265, 768, padding_idx=1)"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"tweetsum_train_dataset = tweetsum_train_clean[\"train\"].map(lambda x: preprocessData(x, tokenizer), batched=True)\ntweetsum_validation_dataset = tweetsum_validate_clean[\"validation\"].map(lambda x: preprocessData(x, tokenizer), batched=True)\ntweetsum_test_dataset = tweetsum_test_clean[\"test\"].map(lambda x: preprocessData(x, tokenizer), batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T11:41:23.275330Z","iopub.execute_input":"2024-11-27T11:41:23.275675Z","iopub.status.idle":"2024-11-27T11:41:24.256148Z","shell.execute_reply.started":"2024-11-27T11:41:23.275646Z","shell.execute_reply":"2024-11-27T11:41:24.255240Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/879 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db0cd38bfce048e5afdf5482b8460430"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4117: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/110 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1593048518bf4a37aff3b3307dfbb243"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/110 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcead2ededce430e91a86e906e7f11f6"}},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"tweetsum_train_dataloader = DataLoader(tweetsum_train_dataset, batch_size=8, shuffle=True)\ntweetsum_validation_dataloader = DataLoader(tweetsum_validation_dataset, batch_size=8)\ntweetsum_test_dataloader = DataLoader(tweetsum_test_dataset, batch_size=8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T11:41:26.047719Z","iopub.execute_input":"2024-11-27T11:41:26.048381Z","iopub.status.idle":"2024-11-27T11:41:26.053588Z","shell.execute_reply.started":"2024-11-27T11:41:26.048344Z","shell.execute_reply":"2024-11-27T11:41:26.052676Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"from transformers import BartForConditionalGeneration, TrainingArguments, Trainer, EarlyStoppingCallback\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results/fine-tuned-model\",\n    evaluation_strategy=\"steps\",  \n    save_strategy=\"steps\",        \n    learning_rate=5e-5,\n    weight_decay= 0.01,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=10,\n    save_total_limit=2,\n    load_best_model_at_end=True,  \n    metric_for_best_model=\"eval_loss\", \n    greater_is_better=False,  \n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model_pretrained,\n    args=training_args,\n    train_dataset=tweetsum_train_dataset,\n    eval_dataset=tweetsum_validation_dataset,\n    tokenizer=tokenizer_pretrained,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n)\n\n# Train the model\ntrainer.train()\n\n# Save the fine-tuned model and tokenizer\nmodel.save_pretrained(\"./finetuned_bart_tweetsum\")\ntokenizer.save_pretrained(\"./finetuned_bart_tweetsum\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T11:41:26.858014Z","iopub.execute_input":"2024-11-27T11:41:26.858636Z","iopub.status.idle":"2024-11-27T11:55:26.465907Z","shell.execute_reply.started":"2024-11-27T11:41:26.858601Z","shell.execute_reply":"2024-11-27T11:55:26.465038Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1100' max='1100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1100/1100 13:57, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.701100</td>\n      <td>0.647596</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.526400</td>\n      <td>0.627805</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"('./finetuned_bart_tweetsum/tokenizer_config.json',\n './finetuned_bart_tweetsum/special_tokens_map.json',\n './finetuned_bart_tweetsum/vocab.json',\n './finetuned_bart_tweetsum/merges.txt',\n './finetuned_bart_tweetsum/added_tokens.json',\n './finetuned_bart_tweetsum/tokenizer.json')"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# Model evaluating using ROUGE\nfrom evaluate import load\nimport torch\n\n# Load ROUGE metric\nrouge = load(\"rouge\")\n\n# Function to generate predictions\ndef generate_predictions(model, tokenizer, dataset):\n    predictions = []\n    references = []\n\n    for example in dataset:\n        # Prepare the input dialogue\n        inputs = tokenizer(\n            example[\"dialogue\"], \n            return_tensors=\"pt\", \n            max_length=512, \n            truncation=True, \n            padding=\"max_length\"\n        )\n        \n        # Move inputs to GPU if available\n        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()} if torch.cuda.is_available() else inputs\n        \n        # Generate summary\n        with torch.no_grad():\n            outputs = model.generate(\n                input_ids=inputs[\"input_ids\"], \n                attention_mask=inputs[\"attention_mask\"], \n                max_length=128, \n                min_length=30, \n                do_sample=False\n            )\n        \n        # Decode the generated summary\n        generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        # Append generated summary and reference summary\n        predictions.append(generated_summary)\n        references.append(example[\"summary\"])\n    \n    return predictions, references\n\n# Generate predictions and references\ntest_predictions, test_references = generate_predictions(model_pretrained, tokenizer_pretrained, tweetsum_validation_dataloader)\n\n# Compute ROUGE scores\nrouge_results = rouge.compute(predictions=test_predictions, references=test_references)\n\n# Print ROUGE scores\nprint(\"ROUGE Scores:\")\nfor key, value in rouge_results.items():\n    print(f\"{key}: {value:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T11:55:26.467220Z","iopub.execute_input":"2024-11-27T11:55:26.467494Z","iopub.status.idle":"2024-11-27T11:55:42.476989Z","shell.execute_reply.started":"2024-11-27T11:55:26.467464Z","shell.execute_reply":"2024-11-27T11:55:42.476140Z"}},"outputs":[{"name":"stdout","text":"ROUGE Scores:\nrouge1: 0.4495\nrouge2: 0.2005\nrougeL: 0.3739\nrougeLsum: 0.3707\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}