{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6004344,"sourceType":"datasetVersion","datasetId":3438844},{"sourceId":10000750,"sourceType":"datasetVersion","datasetId":6155682}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\n\nsamsum_train_dataset = load_dataset(\"csv\", data_files={\"train\": \"/kaggle/input/samsum-dataset-text-summarization/samsum-test.csv\"})\nsamsum_test_dataset = load_dataset(\"csv\", data_files={\"test\": \"/kaggle/input/samsum-dataset-text-summarization/samsum-test.csv\"})\nsamsum_validate_dataset = load_dataset(\"csv\", data_files={\"validation\": \"/kaggle/input/samsum-dataset-text-summarization/samsum-validation.csv\"})\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:31:01.605496Z","iopub.execute_input":"2024-11-24T16:31:01.605930Z","iopub.status.idle":"2024-11-24T16:31:03.592199Z","shell.execute_reply.started":"2024-11-24T16:31:01.605892Z","shell.execute_reply":"2024-11-24T16:31:03.591265Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47abcc9425f7442098bcbaff91b0beb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4936f38369224ae5b8e733096023dc63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f2167b706dc43dc99805413434c667b"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"samsum_train_dataset[\"train\"][1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:31:05.908436Z","iopub.execute_input":"2024-11-24T16:31:05.908848Z","iopub.status.idle":"2024-11-24T16:31:05.918090Z","shell.execute_reply.started":"2024-11-24T16:31:05.908807Z","shell.execute_reply":"2024-11-24T16:31:05.917056Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"{'id': '13729565',\n 'dialogue': \"Eric: MACHINE!\\r\\nRob: That's so gr8!\\r\\nEric: I know! And shows how Americans see Russian ;)\\r\\nRob: And it's really funny!\\r\\nEric: I know! I especially like the train part!\\r\\nRob: Hahaha! No one talks to the machine like that!\\r\\nEric: Is this his only stand-up?\\r\\nRob: Idk. I'll check.\\r\\nEric: Sure.\\r\\nRob: Turns out no! There are some of his stand-ups on youtube.\\r\\nEric: Gr8! I'll watch them now!\\r\\nRob: Me too!\\r\\nEric: MACHINE!\\r\\nRob: MACHINE!\\r\\nEric: TTYL?\\r\\nRob: Sure :)\",\n 'summary': 'Eric and Rob are going to watch a stand-up on youtube.'}"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"print(samsum_test_dataset.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:31:06.896397Z","iopub.execute_input":"2024-11-24T16:31:06.896774Z","iopub.status.idle":"2024-11-24T16:31:06.902752Z","shell.execute_reply.started":"2024-11-24T16:31:06.896735Z","shell.execute_reply":"2024-11-24T16:31:06.901597Z"}},"outputs":[{"name":"stdout","text":"dict_keys(['test'])\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from transformers import pipeline\n\ntext_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:31:07.134385Z","iopub.execute_input":"2024-11-24T16:31:07.134772Z","iopub.status.idle":"2024-11-24T16:31:55.621920Z","shell.execute_reply.started":"2024-11-24T16:31:07.134736Z","shell.execute_reply":"2024-11-24T16:31:55.620788Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0130a13ced1046d2911d1f535c38b553"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bcc8dd665384e65bc74eee1e36776b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4a0c38f8108452dbb73caff55c4cf0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7858de5d33c74c258642f64533682538"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3261d986883b40dcb5f1a7a612c296dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21ac40958a354c65abc323d9f7093dc0"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"text_summarizer(samsum_train_dataset[\"train\"][1][\"dialogue\"], max_length=20, min_length=10, do_sample= False )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:31:55.623537Z","iopub.execute_input":"2024-11-24T16:31:55.624468Z","iopub.status.idle":"2024-11-24T16:31:59.073514Z","shell.execute_reply.started":"2024-11-24T16:31:55.624434Z","shell.execute_reply":"2024-11-24T16:31:59.072474Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"[{'summary_text': 'Rob: Is this his only stand-up? Eric: Sure. Rob: Id'}]"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Fine tune the SamSUM model to improve the summarize performance\n# Add the BART tokenizer and model\nfrom transformers import BartForConditionalGeneration, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\nmodel = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:31:59.074907Z","iopub.execute_input":"2024-11-24T16:31:59.075334Z","iopub.status.idle":"2024-11-24T16:32:02.457992Z","shell.execute_reply.started":"2024-11-24T16:31:59.075288Z","shell.execute_reply":"2024-11-24T16:32:02.456753Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def preprocessData(records, tokenizer, max_length_preprocess=128):\n    sources = records[\"dialogue\"]\n    targets = records[\"summary\"]\n\n    input_encoding = tokenizer(sources, max_length=max_length_preprocess, padding=\"max_length\", truncation=True)\n    with tokenizer.as_target_tokenizer():\n        output_encoding = tokenizer(targets, max_length=max_length_preprocess, padding=\"max_length\", truncation=True)\n\n    # Return as lists to ensure compatibility with DataLoader\n    return {\n        \"input_ids\": input_encoding[\"input_ids\"],\n        \"attention_mask\": input_encoding[\"attention_mask\"],\n        \"labels\": output_encoding[\"input_ids\"],\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:32:02.461247Z","iopub.execute_input":"2024-11-24T16:32:02.461699Z","iopub.status.idle":"2024-11-24T16:32:02.468306Z","shell.execute_reply.started":"2024-11-24T16:32:02.461650Z","shell.execute_reply":"2024-11-24T16:32:02.467113Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"pip install evaluate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:32:02.469460Z","iopub.execute_input":"2024-11-24T16:32:02.469746Z","iopub.status.idle":"2024-11-24T16:32:13.949702Z","shell.execute_reply.started":"2024-11-24T16:32:02.469718Z","shell.execute_reply":"2024-11-24T16:32:13.948038Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"#Apply the preprocessing task\ntrain_data = samsum_train_dataset['train'].map(\n    lambda x: preprocessData(x, tokenizer),\n    batched=True\n)\ntest_data = samsum_test_dataset['test'].map(\n    lambda x: preprocessData(x, tokenizer),\n    batched=True\n)\nvalidate_data = samsum_validate_dataset[\"validation\"].map(\n    lambda x: preprocessData(x, tokenizer),\n    batched=True\n)\ntrain_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\ntest_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\nvalidate_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:32:13.951971Z","iopub.execute_input":"2024-11-24T16:32:13.952460Z","iopub.status.idle":"2024-11-24T16:32:15.991432Z","shell.execute_reply.started":"2024-11-24T16:32:13.952406Z","shell.execute_reply":"2024-11-24T16:32:15.990193Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/819 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab657b5c9a57426a88d609a355779a12"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4117: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/819 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff27f4526d96446ca5d43efdce2af096"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/818 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b559964cf485471a852443f47d4b495d"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"#Convert the preprocessed datasets to PyTorch DataLoader\nfrom transformers import BartForConditionalGeneration, AutoTokenizer, AdamW\nfrom torch.utils.data import DataLoader\nimport evaluate\nimport torch\n\ndef create_dataloader(dataset, batch_size):\n    return DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=lambda x: {\n            \"input_ids\": torch.stack([item[\"input_ids\"] for item in x]),\n            \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in x]),\n            \"labels\": torch.stack([item[\"labels\"] for item in x]),\n        },\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:32:15.992904Z","iopub.execute_input":"2024-11-24T16:32:15.993398Z","iopub.status.idle":"2024-11-24T16:32:16.081120Z","shell.execute_reply.started":"2024-11-24T16:32:15.993342Z","shell.execute_reply":"2024-11-24T16:32:16.079943Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"batch_size = 128\ntrain_dataloader = create_dataloader(train_data, batch_size)\nvalidate_dataloader = create_dataloader(validate_data, batch_size)\ntest_dataloader = create_dataloader(test_data, batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:32:16.082405Z","iopub.execute_input":"2024-11-24T16:32:16.082703Z","iopub.status.idle":"2024-11-24T16:32:16.454382Z","shell.execute_reply.started":"2024-11-24T16:32:16.082674Z","shell.execute_reply":"2024-11-24T16:32:16.453351Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"#Fine-tune the model\nfrom transformers import AdamW\n\n# Define optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Training loop\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nepochs = 10\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n\n    for batch in train_dataloader:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels,\n        )\n        loss = outputs.loss\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n    \n    print(f\"Epoch {epoch + 1}/{epochs} - Loss: {total_loss / len(train_dataloader)}\")\n\n# Save the fine-tuned model\nmodel.save_pretrained(\"./finetuned_bart_samsum\")\ntokenizer.save_pretrained(\"./finetuned_bart_samsum\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:32:16.457398Z","iopub.execute_input":"2024-11-24T16:32:16.458071Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}